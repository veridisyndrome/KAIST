# PIM-based AI accelerator and software stack for efficient LLM inference
## @KAIST (한국과학기술원) | Concurrency & Parallelism Lab

##### Focused on the early stages of research in PIM-based technology for optimizing large-scale model inference, particularly for LLMs. Conducted comprehensive paper reviews, comparative analyses of architectures, reproduced experimental results, and developed new research proposals to address identified gaps.

- **Objective**: To explore the optimization of LLM inference using Process-In-Memory (PIM) architectures.
- **Key Contributions**:
  - Conducted a thorough review and comparison of the **NeuPIMs** and **AttAcc** architectures.
  - Implemented experimental setups to reproduce evaluations and validate the efficiency of PIM in memory-bound tasks such as attention layers in transformers.
  - Proposed new research directions to address the limitations of current PIM systems, focusing on scalability and the need for better support for multi-device workloads.
  - Delivered insights into the future development of AI accelerators specifically designed for LLM inference tasks.

**Supervisors:** Prof. Jeehoon Kang and Haechan An
